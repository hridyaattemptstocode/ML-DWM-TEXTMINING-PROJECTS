{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMsLhTAyf_FB"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Doc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEx9eDUQ1pas",
        "outputId": "87b81523-bc36-45cc-dd47-1791a2f2756d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juKmicBC1pat"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1CLKJI21pat",
        "outputId": "5296e865-3f49-4510-dfe2-0e249d8e5cf6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     C:\\Users\\Aneesh PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to C:\\Users\\Aneesh\n",
            "[nltk_data]    |     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix8ABCWb1pau",
        "outputId": "a9facebe-dc73-4009-b006-539f802a5adf"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'version' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[43mversion\u001b[49m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'version' is not defined"
          ]
        }
      ],
      "source": [
        "--version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1qfEGKMYiKY"
      },
      "outputs": [],
      "source": [
        "file1=open(\"C:\\\\Users\\\\Aneesh PB\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Scripts\\\\Code\\\\TM\\\\Data\\\\Exp 1\\\\DataSets\\\\Doc1.txt\")\n",
        "f1=file1.read()\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3v1XU3_Z2hC",
        "outputId": "6d2fcf47-b27f-407c-f1c8-db2390ecbdb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Aneesh\n",
            "[nltk_data]     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIK990hbfbf3",
        "outputId": "98bd3f0a-2342-4348-bd98-da724d4d153b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CB novels are nightmares now! Now he can reminiscence 5PointSomeone :). better get my bat and play some cricket strokes. Sadly, my arm is hurting from last days cricket match.\n"
          ]
        }
      ],
      "source": [
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lxIpKd8-rl_",
        "outputId": "467859c9-1baa-4ea1-ebc0-014f0bca15c0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7FCoUJ_ZR-E",
        "outputId": "7d6e6316-7cfe-4475-9a95-83c9dfc051ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['CB', 'novels', 'are', 'nightmares', 'now', '!', 'Now', 'he', 'can', 'reminiscence', '5PointSomeone', ':', ')', '.', 'better', 'get', 'my', 'bat', 'and', 'play', 'some', 'cricket', 'strokes', '.', 'Sadly', ',', 'my', 'arm', 'is', 'hurting', 'from', 'last', 'days', 'cricket', 'match', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_word=word_tokenize(f1)\n",
        "print(tokenized_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRRzzJI-Z88A",
        "outputId": "2fef181b-85a5-4783-9cea-5685da135671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'off', 'herself', 'all', 'ourselves', \"shouldn't\", 'did', 'haven', 'when', 'each', 'an', 'below', 'than', 'doing', 'didn', 'once', 'between', 'y', 'more', 'was', 'itself', 'before', 'will', \"should've\", 'its', 'some', 'themselves', 'there', \"don't\", 't', 'with', 'into', 'yourself', \"wouldn't\", 'down', 'those', \"needn't\", 'do', 'hadn', 's', 'same', 're', 'wasn', 'if', 'only', \"weren't\", 'such', 'has', 'had', 'are', 'what', 'ma', 'their', 'mightn', 'most', 'his', 'not', 'mustn', 'we', 'you', 'any', 'm', 'is', 'theirs', 'and', 'don', 'no', 'ours', 'couldn', 'your', 'o', \"haven't\", 'by', 'aren', 'isn', 'up', \"hadn't\", 'just', 'yourselves', 'she', \"that'll\", \"didn't\", 'll', 'does', 'shouldn', 'through', 'nor', 'am', \"you'd\", 'about', 'being', 'where', 'doesn', 'but', 'won', 'himself', 'for', 'have', 'during', 'me', 'too', 'while', \"you've\", 'after', 'them', 'these', 'can', 'now', 'as', \"doesn't\", 'they', 'this', 'against', 'a', 'further', \"hasn't\", 'very', 'wouldn', 'needn', 'it', 'to', 'be', 'of', 'until', 'd', 'ain', \"shan't\", \"wasn't\", 'hers', 'were', 'whom', \"won't\", \"couldn't\", 'her', 'on', \"mightn't\", 'then', 'above', 'having', 'should', 'why', 'so', 'both', 'again', 'i', 'over', 'our', 'how', 'my', 'under', 'been', 'hasn', \"it's\", 'weren', 'other', \"isn't\", 'own', \"she's\", 've', 'because', 'from', 'yours', 'out', 'that', \"aren't\", 'few', 'he', 'at', 'myself', 'in', 'here', 'which', \"mustn't\", 'him', 'or', \"you'll\", 'who', 'the', \"you're\", 'shan'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Aneesh\n",
            "[nltk_data]     PB\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words('english'))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boXcxxkTalKt",
        "outputId": "ef07b88c-de82-495e-8e06-85702c64af06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['CB', 'novels', 'nightmares', '!', 'Now', 'reminiscence', '5PointSomeone', ':', ')', '.', 'better', 'get', 'bat', 'play', 'cricket', 'strokes', '.', 'Sadly', ',', 'arm', 'hurting', 'last', 'days', 'cricket', 'match', '.']\n"
          ]
        }
      ],
      "source": [
        "filtered_tokens=[]\n",
        "for w in tokenized_word:\n",
        "  if w not in stop_words:\n",
        "    filtered_tokens.append(w)\n",
        "print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMGayqTKbGqY",
        "outputId": "d6b87ac2-6473-442e-b163-0b3ffe3ea70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['CB', 'novels', 'nightmares', 'Now', 'reminiscence', '5PointSomeone', 'better', 'get', 'bat', 'play', 'cricket', 'strokes', 'Sadly', 'arm', 'hurting', 'last', 'days', 'cricket', 'match']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "punctuations=list(string.punctuation)\n",
        "filtered_tokens2=[]\n",
        "for s in filtered_tokens:\n",
        "  if s not in punctuations:\n",
        "    filtered_tokens2.append(s)\n",
        "print(filtered_tokens2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDGrQURzdakq",
        "outputId": "5df2bc6e-02f8-4051-f326-50b35de25a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cb', 'novel', 'nightmar', 'now', 'reminisc', '5pointsomeon', 'better', 'get', 'bat', 'play', 'cricket', 'stroke', 'sadli', 'arm', 'hurt', 'last', 'day', 'cricket', 'match']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "ps=PorterStemmer()\n",
        "stemmed_words=[]\n",
        "for w in filtered_tokens2:\n",
        "  stemmed_words.append(ps.stem(w))\n",
        "print(stemmed_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxSrDtqk_RU"
      },
      "source": [
        "Doc2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ9-BnrjlBrn"
      },
      "outputs": [],
      "source": [
        "file2=open('/content/drive/MyDrive/S7/Text Mining/Exp 1/DataSets/Doc2.txt')\n",
        "f2=file2.read()\n",
        "file2.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-b8zqSil0_w",
        "outputId": "e5f197d9-9e28-4379-d80e-00107c3c72fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rocky7 was a Horrible movie, not impressed at all. If they get back their money, it is a stroke of luck!\n"
          ]
        }
      ],
      "source": [
        "print(f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR9dMuPumBge",
        "outputId": "5b6e193a-eb58-4cbc-ba7b-fad0d56b30c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Rocky7', 'was', 'a', 'Horrible', 'movie', ',', 'not', 'impressed', 'at', 'all', '.', 'If', 'they', 'get', 'back', 'their', 'money', ',', 'it', 'is', 'a', 'stroke', 'of', 'luck', '!']\n"
          ]
        }
      ],
      "source": [
        "tokenized_wordf2=word_tokenize(f2)\n",
        "print(tokenized_wordf2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LnIpSh_mNmk",
        "outputId": "5f872c85-ac71-4208-f343-b805142c82a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Rocky7', 'Horrible', 'movie', ',', 'impressed', '.', 'If', 'get', 'back', 'money', ',', 'stroke', 'luck', '!']\n"
          ]
        }
      ],
      "source": [
        "filtered_tokensf2=[]\n",
        "for w in tokenized_wordf2:\n",
        "  if w not in stop_words:\n",
        "    filtered_tokensf2.append(w)\n",
        "print(filtered_tokensf2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfRrDi7Wmq2j",
        "outputId": "b8862a3f-98f2-4e08-90cc-6286d918a29c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Rocky7', 'Horrible', 'movie', 'impressed', 'If', 'get', 'back', 'money', 'stroke', 'luck']\n"
          ]
        }
      ],
      "source": [
        "filtered_tokensf2p=[]\n",
        "for w in filtered_tokensf2:\n",
        "  if w not in punctuations:\n",
        "    filtered_tokensf2p.append(w)\n",
        "print(filtered_tokensf2p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-YN5GYjnEg4",
        "outputId": "c7675b23-dd44-4af1-c4c8-ec58dd6251a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['rocky7', 'horribl', 'movi', 'impress', 'if', 'get', 'back', 'money', 'stroke', 'luck']\n"
          ]
        }
      ],
      "source": [
        "stemmed_wordsf2=[]\n",
        "for w in filtered_tokensf2p:\n",
        "  stemmed_wordsf2.append(ps.stem(w))\n",
        "print(stemmed_wordsf2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGxCeS8koGZV"
      },
      "source": [
        "Doc3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai2a6D8ipgZz"
      },
      "outputs": [],
      "source": [
        "file3=open('/content/drive/MyDrive/S7/Text Mining/Exp 1/DataSets/Doc3.txt')\n",
        "f3=file3.read()\n",
        "file3.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46FiZE8TppYM",
        "outputId": "662e50b3-a966-4be3-a779-5115b4486eaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "novel ikigai is such a soothing read. It talks about crickets in the green field. This novel has a lot of reminiscence about my childhood!!!! \n"
          ]
        }
      ],
      "source": [
        "print(f3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbjWdDE0ppau",
        "outputId": "cc18a3fc-df42-4bc5-ae9f-9e9bc067981d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['novel', 'ikigai', 'is', 'such', 'a', 'soothing', 'read', '.', 'It', 'talks', 'about', 'crickets', 'in', 'the', 'green', 'field', '.', 'This', 'novel', 'has', 'a', 'lot', 'of', 'reminiscence', 'about', 'my', 'childhood', '!', '!', '!', '!']\n"
          ]
        }
      ],
      "source": [
        "tokenized_wordf3=word_tokenize(f3)\n",
        "print(tokenized_wordf3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sevsx-nXppeM",
        "outputId": "b48fda7e-6ab1-43b7-dc8a-9fa683eaf0a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['novel', 'ikigai', 'soothing', 'read', '.', 'It', 'talks', 'crickets', 'green', 'field', '.', 'This', 'novel', 'lot', 'reminiscence', 'childhood', '!', '!', '!', '!']\n"
          ]
        }
      ],
      "source": [
        "filtered_tokensf3=[]\n",
        "for w in tokenized_wordf3:\n",
        "  if w not in stop_words:\n",
        "    filtered_tokensf3.append(w)\n",
        "print(filtered_tokensf3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Td1dos8p4Dx",
        "outputId": "7565d3ad-20e7-4fe9-8fcd-cd2ac5813f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['novel', 'ikigai', 'soothing', 'read', 'It', 'talks', 'crickets', 'green', 'field', 'This', 'novel', 'lot', 'reminiscence', 'childhood']\n"
          ]
        }
      ],
      "source": [
        "filtered_tokensf3p=[]\n",
        "for w in filtered_tokensf3:\n",
        "  if w not in punctuations:\n",
        "    filtered_tokensf3p.append(w)\n",
        "print(filtered_tokensf3p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydrEfqMQp4O-",
        "outputId": "619f5d46-f838-4bd7-9adb-a6410355963c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['novel', 'ikigai', 'sooth', 'read', 'it', 'talk', 'cricket', 'green', 'field', 'thi', 'novel', 'lot', 'reminisc', 'childhood']\n"
          ]
        }
      ],
      "source": [
        "stemmed_wordsf3=[]\n",
        "for w in filtered_tokensf3p:\n",
        "  stemmed_wordsf3.append(ps.stem(w))\n",
        "print(stemmed_wordsf3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMzhtCG3qMFz"
      },
      "source": [
        "Doc4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAGicDqIp4Sl"
      },
      "outputs": [],
      "source": [
        "file4=open('/content/drive/MyDrive/S7/Text Mining/Exp 1/DataSets/Doc4.txt')\n",
        "f4=file4.read()\n",
        "file4.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQIu_utfq9q4",
        "outputId": "678a3ca5-a46b-4040-9f1d-35c53d359e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have an old theatre out here with a lot of bats inside. Not an ideal place to visit in these covid or nipah times. I Should arm myself with a mask.\n"
          ]
        }
      ],
      "source": [
        "print(f4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TILNgU3GrAn2",
        "outputId": "d8767245-8870-4cee-e5cc-eb0aee76d010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We', 'have', 'an', 'old', 'theatre', 'out', 'here', 'with', 'a', 'lot', 'of', 'bats', 'inside', '.', 'Not', 'an', 'ideal', 'place', 'to', 'visit', 'in', 'these', 'covid', 'or', 'nipah', 'times', '.', 'I', 'Should', 'arm', 'myself', 'with', 'a', 'mask', '.']\n"
          ]
        }
      ],
      "source": [
        "tokenized_wordf4=word_tokenize(f4)\n",
        "print(tokenized_wordf4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbCqu21XrAx7",
        "outputId": "74e762d1-c14b-46c6-987e-e60accf11532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We', 'old', 'theatre', 'lot', 'bats', 'inside', '.', 'Not', 'ideal', 'place', 'visit', 'covid', 'nipah', 'times', '.', 'I', 'Should', 'arm', 'mask', '.']\n"
          ]
        }
      ],
      "source": [
        "filtered_tokensf4=[]\n",
        "for w in tokenized_wordf4:\n",
        "  if w not in stop_words:\n",
        "    filtered_tokensf4.append(w)\n",
        "print(filtered_tokensf4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RQXzSGyrPdG",
        "outputId": "f273e984-f844-4fc5-9f0a-0e5f25489fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We', 'old', 'theatre', 'lot', 'bats', 'inside', 'Not', 'ideal', 'place', 'visit', 'covid', 'nipah', 'times', 'I', 'Should', 'arm', 'mask']\n"
          ]
        }
      ],
      "source": [
        "filtered_tokensf4p=[]\n",
        "for w in filtered_tokensf4:\n",
        "  if w not in punctuations:\n",
        "    filtered_tokensf4p.append(w)\n",
        "print(filtered_tokensf4p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MmuXEFMrPoQ",
        "outputId": "36b22259-1f7b-4c1e-8a7c-96962f7b8613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['we', 'old', 'theatr', 'lot', 'bat', 'insid', 'not', 'ideal', 'place', 'visit', 'covid', 'nipah', 'time', 'i', 'should', 'arm', 'mask']\n"
          ]
        }
      ],
      "source": [
        "stemmed_wordsf4=[]\n",
        "for w in filtered_tokensf4p:\n",
        "  stemmed_wordsf4.append(ps.stem(w))\n",
        "print(stemmed_wordsf4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4y-Dk2JtXDI"
      },
      "source": [
        "POS Tagging-Doc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NodwlQBOrPrr",
        "outputId": "8ab90f62-c69a-41ee-e47f-354351301e84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens :  ['CB', 'novels', 'are', 'nightmares', 'now', '!', 'Now', 'he', 'can', 'reminiscence', '5PointSomeone', ':', ')', '.', 'better', 'get', 'my', 'bat', 'and', 'play', 'some', 'cricket', 'strokes', '.', 'Sadly', ',', 'my', 'arm', 'is', 'hurting', 'from', 'last', 'days', 'cricket', 'match', '.']\n",
            "POS Tag :  [('CB', 'NNP'), ('novels', 'NNS'), ('are', 'VBP'), ('nightmares', 'NNS'), ('now', 'RB'), ('!', '.'), ('Now', 'RB'), ('he', 'PRP'), ('can', 'MD'), ('reminiscence', 'VB'), ('5PointSomeone', 'CD'), (':', ':'), (')', ')'), ('.', '.'), ('better', 'JJR'), ('get', 'VB'), ('my', 'PRP$'), ('bat', 'NN'), ('and', 'CC'), ('play', 'VB'), ('some', 'DT'), ('cricket', 'NN'), ('strokes', 'NNS'), ('.', '.'), ('Sadly', 'RB'), (',', ','), ('my', 'PRP$'), ('arm', 'NN'), ('is', 'VBZ'), ('hurting', 'VBG'), ('from', 'IN'), ('last', 'JJ'), ('days', 'NNS'), ('cricket', 'VBD'), ('match', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "tokensf1=word_tokenize(f1)\n",
        "posf1=pos_tag(tokensf1)\n",
        "print(\"Tokens : \",tokensf1)\n",
        "print(\"POS Tag : \",posf1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRdjnaAp0yps"
      },
      "source": [
        "Word Sense Disambiguation-Lesk Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RF3lMoj07qe",
        "outputId": "c604bc68-6c32-42ce-f81a-173b60b1979b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doc 1 :  CB novels are nightmares now! Now he can reminiscence 5PointSomeone :). better get my bat and play some cricket strokes. Sadly, my arm is hurting from last days cricket match.\n",
            "Doc 2 :  Rocky7 was a Horrible movie, not impressed at all. If they get back their money, it is a stroke of luck!\n",
            "Doc 3 :  novel ikigai is such a soothing read. It talks about crickets in the green field. This novel has a lot of reminiscence about my childhood!!!! \n",
            "Doc 4 :  We have an old theatre out here with a lot of bats inside. Not an ideal place to visit in these covid or nipah times. I Should arm myself with a mask.\n"
          ]
        }
      ],
      "source": [
        "print(\"Doc 1 : \",f1)\n",
        "print(\"Doc 2 : \",f2)\n",
        "print(\"Doc 3 : \",f3)\n",
        "print(\"Doc 4 : \",f4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s3e49tD1ajf"
      },
      "source": [
        "'reminiscence' in Doc 1 and Doc 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUX477G82cQR",
        "outputId": "cefeedfb-e49d-4f7c-c028-4f2ac762c68e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee6DxVbE1m_Q",
        "outputId": "331c8e22-8aed-4986-ecd1-a65f197ed55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word :  Synset('reminiscence.n.01')\n",
            "Definition :  a mental impression retained and recalled from the past\n",
            "Word :  Synset('reminiscence.n.01')\n",
            "Definition :  a mental impression retained and recalled from the past\n"
          ]
        }
      ],
      "source": [
        "from nltk.wsd import lesk\n",
        "a1=lesk(word_tokenize('Now he can reminiscence 5PointSomeone :)'),'reminiscence')\n",
        "print(\"Word : \",a1 )\n",
        "print(\"Definition : \",a1.definition())\n",
        "a3=lesk(word_tokenize('This novel has a lot of reminiscence about my childhood!!!!'),'reminiscence')\n",
        "print(\"Word : \",a3 )\n",
        "print(\"Definition : \",a3.definition())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fph26sqE4A0K"
      },
      "source": [
        "'bats' in Doc 1 and Doc 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMcYtWgc4GX-",
        "outputId": "50ce8c5b-e0eb-464f-d58e-438cb7159421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word :  Synset('cricket_bat.n.01')\n",
            "Definition :  the club used in playing cricket\n",
            "Word :  Synset('squash_racket.n.01')\n",
            "Definition :  a small racket with a long handle used for playing squash\n"
          ]
        }
      ],
      "source": [
        "from nltk.wsd import lesk\n",
        "a1=lesk(word_tokenize('better get my bat and play some cricket strokes'),'bat')\n",
        "print(\"Word : \",a1 )\n",
        "print(\"Definition : \",a1.definition())\n",
        "a4=lesk(word_tokenize(' We have an old theatre out here with a lot of bats inside.'),'bats')\n",
        "print(\"Word : \",a4 )\n",
        "print(\"Definition : \",a4.definition())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRmAlHv6cIK"
      },
      "source": [
        "'strokes' in Doc 1 and Doc 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wdStbLX6lqS",
        "outputId": "fca33962-8b9b-4f3f-bb8f-64dbd76bf175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word :  Synset('stroke.v.01')\n",
            "Definition :  touch lightly and repeatedly, as with brushing motions\n",
            "Word :  Synset('throw.n.03')\n",
            "Definition :  the maximum movement available to a pivoted or reciprocating piece by a cam\n"
          ]
        }
      ],
      "source": [
        "from nltk.wsd import lesk\n",
        "a1=lesk(word_tokenize(f1),'strokes')\n",
        "print(\"Word : \",a1 )\n",
        "print(\"Definition : \",a1.definition())\n",
        "a2=lesk(word_tokenize('f2'),'stroke')\n",
        "print(\"Word : \",a2 )\n",
        "print(\"Definition : \",a2.definition())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxInc-la7EPA"
      },
      "source": [
        "'arm' in Doc 1 and Doc 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ddCVq0a7Elm",
        "outputId": "41d7caf4-e154-453d-fccc-52d2b2c16e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word :  Synset('sleeve.n.01')\n",
            "Definition :  the part of a garment that is attached at the armhole and that provides a cloth covering for the arm\n",
            "Word :  Synset('arm.n.04')\n",
            "Definition :  the part of an armchair or sofa that supports the elbow and forearm of a seated person\n"
          ]
        }
      ],
      "source": [
        "from nltk.wsd import lesk\n",
        "a1=lesk(word_tokenize(f1),'arm')\n",
        "print(\"Word : \",a1 )\n",
        "print(\"Definition : \",a1.definition())\n",
        "a4=lesk(word_tokenize(f4),'arm')\n",
        "print(\"Word : \",a4 )\n",
        "print(\"Definition : \",a4.definition())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izem74Rb8wCc"
      },
      "source": [
        "Geeks- Lesk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z74HqtkP8yyC"
      },
      "outputs": [],
      "source": [
        "\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sVSuscO88YG"
      },
      "outputs": [],
      "source": [
        "def get_semantic(seq, key_word):\n",
        "\n",
        "      # Tokenization of the sequence\n",
        "    temp = word_tokenize(seq)\n",
        "\n",
        "    # Retrieving the definition\n",
        "    # of the tokens\n",
        "    temp = lesk(temp, key_word)\n",
        "    return temp.definition()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdTnQnms9MS9",
        "outputId": "3222a445-e5d8-4aef-d360-59de2b0749a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the club used in playing cricket\n",
            "beat thoroughly and conclusively in a competition or fight\n"
          ]
        }
      ],
      "source": [
        "keyword='bats'\n",
        "print(get_semantic(f1,keyword))\n",
        "print(get_semantic(f4,keyword))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}